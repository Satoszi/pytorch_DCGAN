{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mateusz\\anaconda3\\envs\\pytorch_11_2022\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self,nz, filters):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gan = nn.Sequential(\n",
    "            self._block(nz        , 16*filters, 4, 1, 0, bias=False), # out 4x4\n",
    "            self._block(16*filters, 8*filters , 4, 2, 1, bias=False), # out 8x8\n",
    "            self._block(8*filters , 4*filters , 4, 2, 1, bias=False), # out 16x16\n",
    "            self._block(4*filters , 2*filters , 4, 2, 1, bias=False), # out 32x32\n",
    "            self._block(2*filters , 3         , 4, 2, 1, bias=False, out=True), # out 64x64\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding, bias, out=False):\n",
    "        return nn.Sequential(            \n",
    "            nn.ConvTranspose2d(in_channels=in_channels,\n",
    "                               out_channels=out_channels, \n",
    "                               kernel_size=kernel_size,\n",
    "                               stride=stride,\n",
    "                               padding=padding, \n",
    "                               bias=bias),\n",
    "            \n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.Tanh() if out else nn.ReLU()\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.gan( x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, filters):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.gan = nn.Sequential(\n",
    "            self._block(3         , 2*filters, 4, 2, 0, bias=False), # out 32x32 #Shouldn't be batchnorm\n",
    "            self._block(2*filters, 4*filters , 4, 2, 1, bias=False), # out 16x16\n",
    "            self._block(4*filters , 8*filters , 4, 2, 1, bias=False), # out 8x8\n",
    "            self._block(8*filters , 16*filters , 4, 2, 1, bias=False), # out 4x4\n",
    "            self._block(16*filters , 1   , 4, 2, 1, bias=False, out=True), # out 1x1 #Shouldn't be batchnorm\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding, bias, out=False):\n",
    "        return nn.Sequential(            \n",
    "            nn.Conv2d(in_channels=in_channels,\n",
    "                               out_channels=out_channels, \n",
    "                               kernel_size=kernel_size,\n",
    "                               stride=stride,\n",
    "                               padding=padding, \n",
    "                               bias=bias),\n",
    "            \n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.Sigmoid() if out else nn.LeakyReLU(0.2)\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.gan( x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    x = torch.randn((5,3,64,64))\n",
    "    disc = Discriminator(16)\n",
    "    print(disc.forward(x).shape)\n",
    "    \n",
    "    x = torch.randn((5,100,1,1))\n",
    "    gen = Generator(100,16)\n",
    "    print(gen.forward(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1, 1, 1])\n",
      "torch.Size([5, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt_2022_11",
   "language": "python",
   "name": "pt_2022_11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
